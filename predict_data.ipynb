{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "##############################\n",
    "from models import deeplab_xception\n",
    "from models import unet\n",
    "from datasets import datasets\n",
    "from utils import manager as mgr\n",
    "from utils import img_utils\n",
    "##############################\n",
    "\n",
    "# set device\n",
    "#device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "exp_name = 'unet_df_030_001'\n",
    "WEIGHTS_PATH = '/home/philipp/Data/weights/'+exp_name+'/'\n",
    "#device = \"cuda:0\"\n",
    "device = \"cpu\"\n",
    "print(device)\n",
    "\n",
    "nr_channels = 7\n",
    "nr_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights '/home/philipp/Data/weights/unet_df_030_001/unet_weights-19-0.217-0.821.pth'\n",
      "no weights found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philipp/Code/python/edin_prediction/utils/manager.py:56: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  nn.init.kaiming_uniform(m.weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNET(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (upconv4): Sequential(\n",
       "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  )\n",
       "  (upconv3): Sequential(\n",
       "    (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  )\n",
       "  (upconv2): Sequential(\n",
       "    (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  )\n",
       "  (upconv1): Sequential(\n",
       "    (0): Conv2d(128, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(4, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = unet.UNET(in_channels=nr_channels, out_channels=nr_classes)\n",
    "\n",
    "\n",
    "try:\n",
    "    mgr.load_weights(model, WEIGHTS_PATH+'unet_weights-19-0.217-0.821.pth')\n",
    "    #load_weights(model, WEIGHTS_PATH+'latest_5d.pt')\n",
    "    print(\"weights loaded\")\n",
    "except:\n",
    "    model.apply(mgr.weights_init)\n",
    "    print(\"no weights found\")\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing DeepLabv3+ model...\n",
      "Number of classes: 4\n",
      "Output stride: 16\n",
      "Number of Input Channels: 7\n",
      "loading weights '/home/philipp/Data/weights/unet_df_030_001/deeplab_weights-21-0.192-0.818.pth'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8b3f8c7ac486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWEIGHTS_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'deeplab_weights-21-0.192-0.818.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m#load_weights(model, WEIGHTS_PATH+'latest_5d.pt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/python/edin_prediction/utils/manager.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(model, fpath)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading weights '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mstartEpoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'startEpoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/philipp/Data/weights/unet_df_030_001/deeplab_weights-21-0.192-0.818.pth'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8b3f8c7ac486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weights loaded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no weights found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \"\"\"\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \"\"\"\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/python/edin_prediction/utils/manager.py\u001b[0m in \u001b[0;36mweights_init\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "model = deeplab_xception.DeepLabv3_plus(nInputChannels=nr_channels, n_classes=nr_classes, os=16, pretrained=False)\n",
    "\n",
    "try:\n",
    "    mgr.load_weights(model, WEIGHTS_PATH+'deeplab_weights-21-0.192-0.818.pth')\n",
    "    #load_weights(model, WEIGHTS_PATH+'latest_5d.pt')\n",
    "    print(\"weights loaded\")\n",
    "except:\n",
    "    model.apply(mgr.weights_init)\n",
    "    print(\"no weights found\")\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForestDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    '''Characterizes a dataset for PyTorch'''\n",
    "\n",
    "    def __init__(self, path):\n",
    "        '''Initialization'''\n",
    "        # open dataset\n",
    "        self.dset = h5py.File(path, 'r')\n",
    "        self.ortho = self.dset['ortho']\n",
    "        self.dsm = self.dset['dsm']\n",
    "        self.dtm = self.dset['dtm']\n",
    "        self.slope = self.dset['slope']\n",
    "\n",
    "        # set number of samples\n",
    "        self.dataset_size = 10679\n",
    "\n",
    "        ## TODO:\n",
    "        # make means and stds load from hdf5\n",
    "        self.means_tams = np.array([56.12055784563426, 62.130400134006976, 53.03228547781888, 119.50916281232037], dtype='float32')\n",
    "        self.stds_tams = np.array([30.37628560708646, 30.152693706272483, 23.13718651792004, 49.301477498205074], dtype='float32')\n",
    "\n",
    "        self.means_dsm = np.array([13.45]).astype(np.float32)\n",
    "        self.stds_dsm = np.array([10.386633252098674]).astype(np.float32)\n",
    "\n",
    "        self.means_dtm = np.array([1446.0]).astype(np.float32)\n",
    "        self.stds_dtm = np.array([271.05322202384195]).astype(np.float32)\n",
    "\n",
    "        self.means_slope = np.array([22.39]).astype(np.float32)\n",
    "        self.stds_slope = np.array([11.69830556896441]).astype(np.float32)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Denotes the total number of samples'''\n",
    "        return self.dataset_size\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''Generates one sample of data'''\n",
    "\n",
    "        # depending on data change mean and std\n",
    "        means = self.means_tams\n",
    "        stds = self.stds_tams\n",
    "\n",
    "        # Load data and get label\n",
    "        X_ortho = (torch.tensor(self.ortho[index], \\\n",
    "            dtype=torch.float32).permute(2, 0, 1) - \\\n",
    "            means[:, np.newaxis, np.newaxis]) / stds[:, np.newaxis, np.newaxis]\n",
    "        X_dsm = (torch.tensor(self.dsm[index], \\\n",
    "            dtype=torch.float32).permute(2, 0, 1) - self.means_dsm) / self.stds_dsm\n",
    "        X_dtm = (torch.tensor(self.dtm[index], \\\n",
    "            dtype=torch.float32).permute(2, 0, 1) - self.means_dtm) / self.stds_dtm\n",
    "        X_slope = (torch.tensor(self.slope[index], \\\n",
    "            dtype=torch.float32).permute(2, 0, 1) - self.means_slope) / self.stds_slope\n",
    "\n",
    "        X = torch.cat((X_ortho, X_dsm, X_dtm, X_slope),0)\n",
    "\n",
    "        return X #torch.from_numpy(y).permute(2, 0, 1)\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        ''' closes the hdf5 file'''\n",
    "        self.dset.close()\n",
    "\n",
    "\n",
    "    def show_item(self, index):\n",
    "        '''shows the data'''\n",
    "        #plt.imshow(np.array(self.ground_truth[index]))\n",
    "\n",
    "        fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "        dic_data = {'RGB' : [np.array(self.ortho[index][:,:,:3]), [0.1, 0.3, 0.5, 0.7]], \\\n",
    "        'CIR' : [np.array(np.roll(self.ortho[index], 1, axis=2)[:,:,:3]), [0.1, 0.3, 0.5, 0.7]], \\\n",
    "        'DSM' : [np.array(self.dsm[index].astype('f')), [10, 20, 30]], \\\n",
    "        'DTM' : [np.array(self.dtm[index].astype('f')), [10, 20, 30]], \\\n",
    "        'Slope' : [np.array(self.slope[index].astype('f')), [10, 20, 30]]}\n",
    "\n",
    "        for i, key in enumerate(dic_data):\n",
    "            ax = fig.add_subplot(2, 3, i+1)\n",
    "            imgplot = plt.imshow(dic_data[key][0])\n",
    "            ax.set_title(key)\n",
    "            plt.colorbar(ticks=dic_data[key][1], orientation='horizontal')\n",
    "            plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the dataset\n",
    "#path_dataset = \"/home/philipp/Data/dataset_256_df_2.h5\"\n",
    "path_dataset = \"/media/philipp/DATA/dataset/dataset_512_df_prediction.h5\"\n",
    "#path_dataset = \"/media/philipp/DATA/dataset/dataset_256_df_2.h5\"\n",
    "\n",
    "# open dataset\n",
    "dataset = ForestDataset(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quatering(dataset, idx_start, idx_end):\n",
    "    ## prepare input data\n",
    "    # cut into 4 quaters = 1 x 7x512x512 -> 4 x 7x256x256\n",
    "    n = idx_end - idx_start\n",
    "    store = torch.zeros((n*4,7,256,256),dtype=torch.float32)\n",
    "\n",
    "    for j,i in enumerate(range(idx_start, idx_end)):\n",
    "        store[j*4] = dataset[i][:,:256,:256]\n",
    "        store[j*4+1] = dataset[i][:,256:,:256]\n",
    "        store[j*4+2] = dataset[i][:,:256,256:]\n",
    "        store[j*4+3] = dataset[i][:,256:,256:] \n",
    "    return store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(store):\n",
    "    # get predictions\n",
    "    with torch.no_grad():\n",
    "        output = model(store)\n",
    "    pred_256 = mgr.get_predictions(output).numpy()\n",
    "    return pred_256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(pred_256, idx_start, idx_end):\n",
    "    ## prepare output data\n",
    "    # merge into 4 quaters = 4 x 256x256 -> 1 x 512x512\n",
    "    n = idx_end - idx_start\n",
    "    pred_512 = np.zeros((n,512,512), dtype=np.int8)\n",
    "    \n",
    "    for i in range(n):\n",
    "        j = i*4\n",
    "        pred_512[i,:256,:256] = pred_256[j]\n",
    "        pred_512[i,256:,:256] = pred_256[j+1]\n",
    "        pred_512[i,:256,256:] = pred_256[j+2]\n",
    "        pred_512[i,256:,256:] = pred_256[j+3]\n",
    "    return pred_512    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 512, 512)\n",
      "0 40\n",
      "40 80\n",
      "80 120\n",
      "120 160\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "end = 160\n",
    "batch = 40\n",
    "\n",
    "p = np.zeros((end-start,512,512), dtype=np.int8)\n",
    "print(p.shape)\n",
    "\n",
    "counter = start\n",
    "\n",
    "for i in range((end-start) // batch):\n",
    "    \n",
    "    store = print(counter, counter+batch)\n",
    "    \n",
    "    store = quatering(dataset=dataset, idx_start=counter, idx_end=counter+batch)\n",
    "    pred_256 = predict(store)\n",
    "    pred_512 = merge(pred_256, counter, counter+batch)\n",
    "    \n",
    "    p[i*batch:(i+1)*batch] = pred_512\n",
    "    \n",
    "    counter += batch\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 512, 512)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pred.npy', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare input data\n",
    "# cut into 4 quaters = 1 x 7x512x512 -> 4 x 7x256x256\n",
    "\n",
    "start = 0\n",
    "end = 40\n",
    "\n",
    "n = end - start\n",
    "store = torch.zeros((n*4,7,256,256),dtype=torch.float32)\n",
    "\n",
    "for i,j in enumerate(range(0,n*4,4)):\n",
    "    store[j] = dataset[i][:,:256,:256]\n",
    "    store[j+1] = dataset[i][:,256:,:256]\n",
    "    store[j+2] = dataset[i][:,:256,256:]\n",
    "    store[j+3] = dataset[i][:,256:,256:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "with torch.no_grad():\n",
    "    output = model(store)\n",
    "p = mgr.get_predictions(output).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 256, 256)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare output data\n",
    "# merge into 4 quaters = 4 x 7x256x256 -> 1 x 7x512x512\n",
    "\n",
    "pred = np.zeros((n,512,512), dtype=np.int8)\n",
    "\n",
    "for i,j in enumerate(range(0,n*4,4)):\n",
    "    pred[i,:256,:256] = p[j]\n",
    "    pred[i,256:,:256] = p[j+1]\n",
    "    pred[i,:256,256:] = p[j+2]\n",
    "    pred[i,256:,256:] = p[j+3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 512, 512)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 119.50916281232037\n",
    "std = 49.301477498205074\n",
    "\n",
    "i = 0\n",
    "l = 10000\n",
    "nirs = []\n",
    "\n",
    "for i in range(20):\n",
    "    # grab data\n",
    "    x = torch.stack([dataset[i][3], \\\n",
    "                    dataset[i+1*l][3], \\\n",
    "                    dataset[i+2*l][3], \\\n",
    "                    dataset[i+3*l][3]])\n",
    "    \n",
    "    # reconstruct tile\n",
    "    merged_512 = np.ones((512, 512))\n",
    "    # paste predicted data into array\n",
    "    merged_512[:256,:256] = dataset[i][3]\n",
    "    merged_512[256:,:256] = dataset[i+1*l][3]\n",
    "    merged_512[:256,256:] = dataset[i+2*l][3]\n",
    "    merged_512[256:,256:] = dataset[i+3*l][3]\n",
    "    \n",
    "    merged_512 = std * merged_512 + mean\n",
    "    \n",
    "    nirs.append(merged_512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nirs = np.array(nirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('nir.npy', nirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pred.npy', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert to geotiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from osgeo import gdal\n",
    "from osgeo import gdal_array\n",
    "from osgeo import osr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array2raster(newRasterfn, dataset, array, dtype):\n",
    "    \"\"\"\n",
    "    save GTiff file from numpy.array\n",
    "    input:\n",
    "        newRasterfn: save file name\n",
    "        dataset : original tif file\n",
    "        array : numpy.array\n",
    "        dtype: Byte or Float32.\n",
    "    \"\"\"\n",
    "    cols = array.shape[1]\n",
    "    rows = array.shape[0]\n",
    "    originX, pixelWidth, b, originY, d, pixelHeight = dataset.GetGeoTransform() \n",
    "\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "\n",
    "    # set data type to save.\n",
    "    GDT_dtype = gdal.GDT_Unknown\n",
    "    if dtype == \"Byte\": \n",
    "        GDT_dtype = gdal.GDT_Byte\n",
    "    elif dtype == \"Float32\":\n",
    "        GDT_dtype = gdal.GDT_Float32\n",
    "    else:\n",
    "        print(\"Not supported data type.\")\n",
    "\n",
    "    # set number of band.\n",
    "    if array.ndim == 2:\n",
    "        band_num = 1\n",
    "    else:\n",
    "        band_num = array.shape[2]\n",
    "\n",
    "    outRaster = driver.Create(newRasterfn, cols, rows, band_num, GDT_dtype)\n",
    "    outRaster.SetGeoTransform((originX, pixelWidth, 0, originY, 0, pixelHeight))\n",
    "\n",
    "    # Loop over all bands.\n",
    "    for b in range(band_num):\n",
    "        outband = outRaster.GetRasterBand(b + 1)\n",
    "        # Read in the band's data into the third dimension of our array\n",
    "        if band_num == 1:\n",
    "            outband.WriteArray(array)\n",
    "        else:\n",
    "            outband.WriteArray(array[:,:,b])\n",
    "\n",
    "    # setteing srs from input tif file.\n",
    "    prj=dataset.GetProjection()\n",
    "    outRasterSRS = osr.SpatialReference(wkt=prj)\n",
    "    outRaster.SetProjection(outRasterSRS.ExportToWkt())\n",
    "    outband.FlushCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.load('pred.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(dir_img, sort=False):\n",
    "    \"\"\"\n",
    "    find paths for provided data types\n",
    "    inputs:\n",
    "        dir_img (str) : directory path\n",
    "    return:\n",
    "        paths (dictionary) : dictionary containing file paths for each of the data types\n",
    "    \"\"\"\n",
    "\n",
    "    paths = []\n",
    "    # loop over all files found in directory and retrive indices\n",
    "    for file in os.listdir(\"{}\".format(dir_img)):\n",
    "        if file[-4:] == \".tif\":\n",
    "            paths.append(dir_img + file)\n",
    "\n",
    "    if sort:\n",
    "        paths = sorted(paths)\n",
    "    \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input = find_files('/media/philipp/DATA/2018_tamsweg/ortho/', sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/philipp/DATA/2018_tamsweg/ortho/tile_ortho_121889.tif',\n",
       " '/media/philipp/DATA/2018_tamsweg/ortho/tile_ortho_121890.tif',\n",
       " '/media/philipp/DATA/2018_tamsweg/ortho/tile_ortho_122277.tif']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_input[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10679"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(path_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array to geotiff files\n",
    "for i, path_in in enumerate(path_input[:pred.shape[0]]):\n",
    "    path_out = 'foto/pred_{}.tif'.format(i)\n",
    "    dataset = gdal.Open(path_in, gdal.GA_ReadOnly)\n",
    "    array2raster(newRasterfn=path_out, dataset=dataset, array=pred[i], dtype='Byte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mosaik geotiff files\n",
    "\n",
    "def mosaik_raster(input_files, output_file):\n",
    "    \n",
    "    # set path to text file with all tif file paths to be merged\n",
    "    merge_list_path =  '/home/philipp/Code/python/edin_prediction/dtm_merge_list.txt'\n",
    "    # create txt file with all tif files needed\n",
    "    with open(merge_list_path, 'w+') as f:\n",
    "        for name in input_files:\n",
    "            f.write(name + '\\n')\n",
    "\n",
    "    # create string containing bash command\n",
    "    cmd = \"gdal_merge.py -ot Byte -of GTiff -o {} --optfile {}\".format(output_file, merge_list_path)\n",
    "\n",
    "    # execute bash command\n",
    "    os.system(cmd)\n",
    "\n",
    "    print(\"DTM file created - files merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = find_files('/home/philipp/Code/python/edin_prediction/foto/', sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM file created - files merged\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-48db12951790>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/philipp/Code/python/edin_prediction/pred/pred.tif'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmosaik_raster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-5225dd2ac846>\u001b[0m in \u001b[0;36mmosaik_raster\u001b[0;34m(input_files, output_file)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DTM file created - files merged\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtm_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "output_file = '/home/philipp/Code/python/edin_prediction/pred/pred.tif'\n",
    "mosaik_raster(input_files, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_file = '/home/philipp/Code/python/edin_prediction/pred/pred.shp'\n",
    "# create vector (polygons) out of raster\n",
    "cmd = 'gdal_polygonize.py -8 {} -b 1 -f \"ESRI Shapefile\" {} output DF'.format(output_file, vector_file)\n",
    "# execute bash command\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2803.2375"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "42/160*10679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
